# -*- coding: utf-8 -*-
"""02. Text Data

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14gdWJc7Y0qLHMhIZvsdEt2V_K6rC_EvI
"""

import nltk

nltk.download('names')

from nltk.corpus import names

print(names.words()[:5])

print(len(names.words()))

from sklearn.datasets import fetch_20newsgroups
groups = fetch_20newsgroups()

#data : 본문 텍스트로 구성된 리스트
groups.data[0]

# 첫 번째 본문 텍스트의 뉴스 그룹 번호
groups.target[0]

groups.target_names

#텍스트 전처리

nltk.download('punkt')
from nltk import word_tokenize

tokens = word_tokenize("Hello, everyone, Nice to meet you.")
print(tokens)

#품사 태깅

nltk.download('averaged_perceptron_tagger')

pos = nltk.pos_tag(tokens)

print(pos)

#NER

import en_core_web_sm

nlp = en_core_web_sm.load()

doc = nlp('''European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices''')

for entity in doc.ents:
  print('{:12} \t {}'.format(entity.text, entity.label_))

#어간 추출
from nltk.stem.porter import PorterStemmer
porter_stemmer = PorterStemmer()

porter_stemmer.stem('machines')

porter_stemmer.stem('learning')

#표제어 원형 복원

nltk.download('wordnet')

from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

lemmatizer.lemmatize('machines')

lemmatizer.lemmatize('learning')

#텍스트 특성 (Term Frequency)

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.datasets import fetch_20newsgroups

groups = fetch_20newsgroups() #target data

cv = CountVectorizer(max_features=100) #빈도 수 기준 상위 100개 단어 선택
cv_model = cv.fit(groups.data) #전체 데이터셋에 대하여 모델링

print(cv_model.get_feature_names())

cv_result = cv_model.transform(groups.data)

cv_result[0].toarray()

cv = CountVectorizer(max_features=100, stop_words='english')

cv_model = cv.fit(groups.data)

#TF-IDF

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.datasets import fetch_20newsgroups

groups = fetch_20newsgroups()

tfidfv = TfidfVectorizer(max_features=100, stop_words='english')
tfidfv_model = tfidfv.fit(groups.data)

print(tfidfv_model.get_feature_names())

tfidfv_result = tfidfv_model.transform(groups.data)

tfidfv_result[0].toarray()

